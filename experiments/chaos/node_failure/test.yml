---
- hosts: localhost
  connection: local
  gather_facts: False

  vars_files:
    - test_vars.yml
    - /mnt/parameters.yml

  tasks:

    - block:

        - include_tasks: /utils/fcm/create_testname.yml

        - include_tasks: /utils/fcm/update_litmus_result_resource.yml
          vars:
            status: 'SOT'
            chaostype: "node-failure"
            app: ""

        ## DISPLAY APP INFORMATION
        - name: Display the app information passed via the test job
          debug:
            msg:
              - "The application info is as follows:"
              - "Namespace    : {{ namespace }}"
              - "Label        : {{ label }}"

        - include: prerequisites.yml

        - name: Identify the chaos util to be invoked
          template:
            src: chaosutil.j2
            dest: chaosutil.yml

        - include_vars:
            file: chaosutil.yml

        - name: Record the chaos util path
          set_fact:
            chaos_util_path: "/{{ chaosutil }}"

        - name: Identify the data consistency util to be invoked
          template:
            src: data_persistence.j2
            dest: data_persistence.yml

        - include_vars:
            file: data_persistence.yml

        - name: Record the data consistency util path
          set_fact:
            data_consistency_util_path: "{{ consistencyutil }}"
          when: data_persistence != ''

        ## Verify the APPLICATION UNDER TEST PRE CHAOS
        - name: Verify if the application is running.
          include_tasks: /utils/k8s/status_app_pod.yml
          vars:
            app_ns: "{{ namespace }}"
            app_lkey: "{{ label.split('=')[0] }}"
            app_lvalue: "{{ label.split('=')[1] }}"
            delay: '2'
            retries: '60'

        ## Fetch application pod name
        - name: Get application pod name
          shell: >
            kubectl get pod -n {{ namespace }} -l {{ label }} --no-headers -o=custom-columns=NAME:".metadata.name"
          args:
            executable: /bin/bash
          register: app_pod_name

        - name: Record the application pod name
          set_fact:
            application_pod: "{{ app_pod_name.stdout }}"

        - name: Obtain PVC name from the application mount
          shell: >
            kubectl get pods "{{ app_pod_name.stdout }}" -n "{{ namespace }}" 
            -o custom-columns=:.spec.volumes[*].persistentVolumeClaim.claimName --no-headers
          args:
            executable: /bin/bash
          register: pvc

        - name: Obtain the Persistent Volume name
          shell: >
            kubectl get pvc "{{ pvc.stdout }}" -n "{{ namespace }}" --no-headers 
            -o custom-columns=:.spec.volumeName
          args:
            executable: /bin/bash
          register: pv
          failed_when: 'pv.stdout == ""'

        - name: Record the pv name
          set_fact:
            pv_name: "{{ pv.stdout }}"

        ## Generate data on the application
        - name: Generate data on the specified application.
          include: "{{ data_consistency_util_path }}"
          vars:
            status: 'LOAD'
            ns: "{{ namespace }}"
            pod_name: "{{ app_pod_name.stdout }}"
          when: data_persistence != ''

        ## Obtain the node where application pod is running
        - name: Get Application pod Node to perform chaos
          shell: >
            kubectl get pod {{ app_pod_name.stdout }} -n {{ namespace }}
            --no-headers -o custom-columns=:spec.nodeName
          args:
            executable: /bin/bash
          register: app_node

        - name: Record the application pod node name
          set_fact:
            app_node_name: "{{ app_node.stdout }}"

        ## Execute the chaos util to turn off the target node
        - include_tasks: "{{ chaos_util_path }}"
          vars:
            esx_ip: "{{ host_ip }}"
            target_node: "{{ app_node.stdout }}"
            operation: "off"
          when: platform == 'vmware'

        - name: Check the node status
          shell: kubectl get nodes {{ app_node.stdout }} --no-headers
          args:
            executable: /bin/bash
          register: state
          until: "'NotReady' in state.stdout"
          delay: 15
          retries: 30

        # wait 5 minutes
        - name: Wait for the application pod to get evicted
          wait_for:
            timeout: 300

        - block:
          ## Application verification after injecting chaos
          - name: check the application status
            shell: kubectl get pods -n {{ namespace}} -l {{ label }} --no-headers -o custom-columns=:.status.phase
            args:
              executable: /bin/bash
            register: app_status
            until: "'Running' in app_status.stdout"
            delay: 5
            retries: 10
            ignore_errors: true

          - name: Check if the CVRs are in healthy state
            shell: >
              kubectl get cvr -n {{ operator_ns }} -l openebs.io/persistent-volume={{ pv.stdout }}
              -o custom-columns=:.status.phase --no-headers
            args:
              executable: /bin/bash
            register: cvr_status
            until: "((cvr_status.stdout_lines|unique)|length) == 1 and 'Healthy' in cvr_status.stdout"
            retries: 60
            delay: 10
            when: stg_engine == 'cstor'

            ### Due to node shutdown, the application pod will be in containercreating state due to multi-attach error.
            ### As a workaround, the node CR can be deleted which can forcefully remove the terminating pod.

          - name: Delete the node CR
            shell: kubectl delete node {{ app_node.stdout }}
            args:
              executable: /bin/bash
            register: node_status
            failed_when: node_status.rc != 0

            ## After deleting node CR, it will take 6 minutes for the volume to be running
          - name: wait for the application pod to start
            wait_for:
              timeout: 360

          - name: Check if the application pod is rescheduled
            shell: >
              kubectl get pods -n {{ namespace }} -l {{ label }} --no-headers
              -o wide | grep -v {{ app_node.stdout }} | awk '{print $3}'
            args:
              executable: /bin/bash
            register: rescheduled_app_pod_status
            until: "'Running' in rescheduled_app_pod_status.stdout"
            delay: 10
            retries: 45

          when: stg_prov == "openebs.io/provisioner-iscsi" or stg_prov == "cstor.csi.openebs.io"

        - block:

          - name: Check if the new application pod is scheduled after node failure
            shell: >
              kubectl get pods -n {{ namespace }} -l {{ label }} --no-headers | wc -l
            args:
              executable: /bin/bash
            register: app_pod_count
            until: "'2' in app_pod_count.stdout"
            delay: 15
            retries: 30
  
          - name: Get the new application pod name
            shell: > 
              kubectl get pod -n {{ namespace }} -l {{ label }} --no-headers | grep -v Terminating | awk '{print $1}'
            args:
              executable: /bin/bash
            register: new_app_pod_name
  
          - name: Record the new application pod name 
            set_fact:
              new_app_pod: "{{ new_app_pod_name.stdout }}"
           
          - name: Check for the newly created application pod status 
            shell: >
              kubectl get pod {{ new_app_pod }} -n {{ namespace }} --no-headers -o custom-columns=:.status.phase
            args:
              executable: /bin/bash
            register: new_app_pod_status
            failed_when: "'Pending' not in new_app_pod_status.stdout"

          - include_tasks: "{{ chaos_util_path }}"
            vars:
              esx_ip: "{{ host_ip }}"
              target_node: "{{ app_node_name }}"
              operation: "on"
            when: platform == 'vmware'

          - name: Check the node status
            shell: kubectl get node {{ app_node_name }} --no-headers
            args:
              executable: /bin/bash
            register: node_status
            until: "'NotReady' not in node_status.stdout"
            delay: 10
            retries: 30

          - name: verify that previous application pod is successfully deleted
            shell: kubectl get pod -n {{ namespace }} -l {{ label }} --no-headers
            args:
              executable: /bin/bash
            register: app_pod_status
            until: "'{{ application_pod }}' not in app_pod_status.stdout"
            delay: 5
            retries: 40

          - name: Get the IP Address of the node on which application pod is scheduled
            shell: >
              kubectl get nodes {{ app_node_name }} --no-headers -o jsonpath='{.status.addresses[0].address}'
            args:
              executable: /bin/bash
            register: node_ip_address
      
          - name: Record the IP Address of the node on which application pod is scheduled
            set_fact:
              node_ip_add: "{{ node_ip_address.stdout }}"

          - name: Check if zpool is present 
            shell: >
              sshpass -p {{ node_pwd }} ssh -o StrictHostKeyChecking=no {{ user }}@{{ node_ip_add }} "zpool list"
            args:
              executable: /bin/bash
            register: zpool_status          

          - name: Import the zpool after turning on the VM's
            shell: >
              sshpass -p {{ node_pwd }} ssh -o StrictHostKeyChecking=no {{ user }}@{{ node_ip_add }}
              "echo {{ node_pwd }} | sudo -S su -c 'zpool import {{ zpool_name }}'"
            args:
              executable: /bin/bash
            register: status
            failed_when: "status.rc != 0"
            when: "'{{ zpool_name }}' not in zpool_status.stdout"
            
          - name: verify that zfs dataset is available now
            shell: >
              sshpass -p {{ node_pwd }} ssh -o StrictHostKeyChecking=no {{ user }}@{{ node_ip_add }} "zfs list"
            args: 
              executable: /bin/bash
            register: zfs_dataset
            until: "'{{ zpool_name }}/{{ pv_name }}' in zfs_dataset.stdout"
            delay: 10
            retries: 30

          - name: check the newly scheduled application pod status
            shell: kubectl get pod {{ new_app_pod }} -n {{ namespace }} --no-headers -o custom-columns=:.status.phase
            args:
              executable: /bin/bash
            register: new_app_pod_status
            until: "'Running' in new_app_pod_status.stdout"
            delay: 5
            retries: 50

          when: stg_prov == "zfs.csi.openebs.io"

        - block:

            - name: Obtain the rescheduled pod name
              shell: >
                kubectl get pods -n {{ namespace }} -l {{ label }} --no-headers
                -o custom-columns=:metadata.name
              args:
                executable: /bin/bash
              register: rescheduled_app_pod

            - name: Verify application data persistence
              include: "{{ data_consistency_util_path }}"
              vars:
                status: 'VERIFY'
                ns: "{{ namespace }}"
                pod_name: "{{ rescheduled_app_pod.stdout }}"

          when: data_persistence != ''

        - set_fact:
            flag: "Pass"

      rescue:
        - set_fact:
            flag: "Fail"

      always:

        # Turn on the k8s node
        - include_tasks: "{{ chaos_util_path }}"
          vars:
            esx_ip: "{{ host_ip }}"
            target_node: "{{ app_node.stdout }}"
            operation: "on"
          when: 
          - platform == 'vmware'
          - stg_prov == "openebs.io/provisioner-iscsi" or stg_prov == "cstor.csi.openebs.io"

        - include_tasks: /utils/fcm/update_litmus_result_resource.yml
          vars:
            status: 'EOT'
            chaostype: "node-failure"
            app: ""
