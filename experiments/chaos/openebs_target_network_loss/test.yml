---
- hosts: localhost
  connection: local

  vars_files:
    - test_vars.yml
    - /mnt/parameters.yml

  tasks:
    - block:

        - include_tasks: /utils/k8s/application_liveness_check.yml
          when: liveness_label != ''    

        ## DERIVE THE APP STORAGE CLASS AND CHAOS UTIL TO USE

        - include: test_prerequisites.yml

        - include_vars:
            file: data_persistence.yml

        - include_vars:
            file: chaosutil.yml

        - name: Record the chaos util path
          set_fact:
            chaos_util_path: "/chaoslib/{{ chaosutil }}"

        - name: Record the data consistency util path
          set_fact:
            data_consistency_util_path: "{{ consistencyutil }}"
          when: data_persistence != ''

        ## RECORD START-OF-TEST IN LITMUS RESULT CR
        
        - include_tasks: /utils/fcm/create_testname.yml

        - include_tasks: /utils/fcm/update_litmus_result_resource.yml
          vars:
            status: 'SOT'
            chaostype: "{{ chaosutil.split('.')[0] }}"

        ## DISPLAY APP INFORMATION

        - name: Display the app information passed via the test job
          debug:
            msg:
              - "The application info is as follows:"
              - "Namespace    : {{ namespace }}"
              - "Label        : {{ label }}"
              - "PVC          : {{ pvc }}"
              - "StorageClass : {{ sc }}"

        ## PRE-CHAOS APPLICATION LIVENESS CHECK
        - name: Verify that the AUT (Application Under Test) is running
          include_tasks: "/utils/k8s/status_app_pod.yml"
          vars:
            app_ns: "{{namespace}}" 
            app_lkey: "{{ label.split('=')[0] }}"
            app_lvalue: "{{ label.split('=')[1] }}"       
            delay: 5
            retries: 60

        - name: Get application pod name
          shell: >
            kubectl get pods -n {{ namespace }} -l {{ label }} --no-headers
            -o=custom-columns=NAME:".metadata.name"
          args:
            executable: /bin/bash
          register: app_pod_name

        - name: Create some test data
          include: "{{ data_consistency_util_path }}"
          vars:
            status: 'LOAD'
            ns: "{{ namespace }}"
            pod_name: "{{ app_pod_name.stdout }}"
          when: data_persistence != ''

        - name: Derive PV from application PVC 
          shell: >
            kubectl get pvc {{ pvc }}
            -o custom-columns=:spec.volumeName -n {{ namespace }}
            --no-headers
          args:
            executable: /bin/bash
          register: pv

        - name: Pick a cStor target pod belonging to the PV
          shell: >
            kubectl get pods -l openebs.io/target=cstor-target
            -n {{ operator_ns }} --no-headers | grep {{ pv.stdout }}
            | shuf -n1 | awk '{print $1}'
          args:
            executable: /bin/bash
          register: cstor_target_pod
          when: stg_engine == 'cstor'
        
        - block:

            - name: Identify the patch to be invoked
              template:
                src: patch.j2
                dest: patch.yml

            - name: Patching jiva controller deployment to allow security privileged
              shell: >
                kubectl patch deployment {{ pv.stdout }}-ctrl -n {{ operator_ns }}
                --patch "$(cat patch.yml)"
              register: patch_status
              failed_when: "'patched' not in patch_status.stdout"
            
            - name: Wait for 10s post fault injection 
              wait_for:
                timeout: 10

            - name: Identify the jiva controller pod belonging to the PV
              shell: > 
                kubectl get pods -l openebs.io/controller=jiva-controller
                -n {{ operator_ns }} --no-headers | grep {{ pv.stdout }} 
                | awk '{print $1}'
              args:
                executable: /bin/bash
              register: controller_pod

            - name: Check for the jiva controller container status
              shell: >
                kubectl get pods {{ controller_pod.stdout }} -n {{ operator_ns }} 
                -o jsonpath='{.status.containerStatuses[?(@.name=="{{ pv.stdout}}-ctrl-con")].state}' | grep running
              args:
                executable: /bin/bash
              register: container_status
              until: "'running' in container_status.stdout"
              delay: 3
              retries: 20
                                      
          when: 
            - stg_engine == 'jiva'
            - stg_prov == 'openebs.io/provisioner-iscsi'          

        - block:

            - name: Identify the patch to be invoked
              template:
                src: jiva-csi-patch.j2
                dest: jiva-csi-patch.yml

            - name: Patching jiva controller deployment to allow security privileged
              shell: >
                kubectl patch deployment {{ pv.stdout }}-jiva-ctrl -n {{ operator_ns }}
                --patch "$(cat jiva-csi-patch.yml)"
              register: patch_status
              failed_when: "'patched' not in patch_status.stdout"
            
            - name: Wait for 10s post fault injection 
              wait_for:
                timeout: 10

            - name: Identify the jiva controller pod belonging to the PV
              shell: > 
                kubectl get pods -l openebs.io/component=jiva-controller
                -n {{ operator_ns }} --no-headers -o custom-columns=:.metadata.name
                | grep {{ pv.stdout }} 
              args:
                executable: /bin/bash
              register: ctrl_pod

            - name: Check for the jiva controller container status
              shell: >
                kubectl get pods {{ controller_pod.stdout }} -n {{ operator_ns }} 
                -o jsonpath='{.status.containerStatuses[?(@.name=="jiva-controller")].state}' | grep running
              args:
                executable: /bin/bash
              register: container_status
              until: "'running' in container_status.stdout"
              delay: 3
              retries: 20
                                      
          when: 
            - stg_engine == 'jiva'
            - stg_prov == 'jiva.csi.openebs.io'          
    

        ## STORAGE FAULT INJECTION

        - include: "{{ chaos_util_path }}"
          vars:
            app_ns: "{{ namespace }}"
            app_pvc: "{{ pvc }}"
            network_delay: "{{ n_delay }}"
            chaos_duration: "{{ c_duration }}"
          when: 
            - cri == 'docker'
            - stg_prov == 'openebs.io/provisioner-iscsi'

        - include: "{{ chaos_util_path }}"
          vars:
            status: "induce"
            target_pod: "{{ cstor_target_pod.stdout }}"
            operator_namespace: "{{ operator_ns }}"
            containername: "cstor-istgt"
          when: 
            - cri == 'containerd' or cri =='cri-o'  
            - stg_engine == 'cstor'

        - include: "{{ chaos_util_path }}"
          vars:
            status: "induce"
            target_pod: "{{ controller_pod.stdout }}"
            operator_namespace: "{{ namespace }}"
            containername: "{{ pv.stdout }}-ctrl-con"
          when: 
            - cri == 'containerd' or cri =='cri-o'  
            - stg_engine == 'jiva'
            - stg_prov == 'openebs.io/provisioner-iscsi'

        - include: "{{ chaos_util_path }}"
          vars:
            status: "induce"
            target_pod: "{{ ctrl_pod.stdout }}"
            operator_namespace: "{{ operator_ns }}"
            containername: "jiva-controller"
          when:   
            - stg_engine == 'jiva'
            - stg_prov == 'jiva.csi.openebs.io'

        - name: Wait for 240s post fault injection 
          wait_for:
            timeout: 240
          when: cri == 'containerd' or cri =='cri-o' or stg_prov == 'jiva.csi.openebs.io'

        - include: "{{ chaos_util_path }}"
          vars:
            status: "remove"
            target_pod: "{{ cstor_target_pod.stdout }}"
            operator_namespace: "{{ operator_ns }}"
            containername: "cstor-istgt"
          when: 
            - cri == 'containerd' or cri =='cri-o'  
            - stg_engine == 'cstor'

        - include: "{{ chaos_util_path }}"
          vars:
            status: "remove"
            target_pod: "{{ controller_pod.stdout }}"
            operator_namespace: "{{ namespace }}"
            containername: "{{ pv.stdout }}-ctrl-con"
          when: 
            - cri == 'containerd' or cri =='cri-o'  
            - stg_engine == 'jiva'
            - stg_prov == 'openebs.io/provisioner-iscsi'

        - include: "{{ chaos_util_path }}"
          vars:
            status: "remove"
            target_pod: "{{ ctrl_pod.stdout }}"
            operator_namespace: "{{ operator_ns }}"
            containername: "jiva-controller"
          when:   
            - stg_engine == 'jiva'
            - stg_prov == 'jiva.csi.openebs.io'

        ## POST-CHAOS APPLICATION LIVENESS CHECK

        - block:
            - name: Kill the application pod
              shell: >
                kubectl delete pod {{ app_pod_name.stdout }} -n {{ namespace }}
              args:
                executable: /bin/bash

            - name: Verify if the application pod is deleted
              shell: >
                kubectl get pods -n {{ namespace }}
              args:
                executable: /bin/bash
              register: podstatus
              until: '"{{ app_pod_name.stdout }}" not in podstatus.stdout'
              retries: 2
              delay: 150

            - name: Obtain the newly created pod name for application
              shell: >
                kubectl get pods -n {{ namespace }} -l {{ label }} -o jsonpath='{.items[].metadata.name}'
              args:
                executable: /bin/bash
              register: newpod_name

            - name: Checking application pod not in running state
              shell: kubectl get pods -n {{ namespace }} -o jsonpath='{.items[?(@.metadata.name=="{{ newpod_name.stdout }}")].status.containerStatuses[*].state}'
              register: result
              until: "'running' not in result.stdout"
              delay: 2
              retries: 150
          when: stg_prov == 'openebs.io/provisioner-iscsi'

        - block:

            - name: Verify that the volume is healthy
              shell: >
                kubectl get cstorvolume {{ pv.stdout }} -n {{ operator_ns }} 
                --no-headers -o custom-columns=:.status.phase
              args:
                executable: /bin/bash
              register: Status
              until: "'Healthy' in Status.stdout"
              delay: 5
              retries: 60

            - name: Verify that the AUT (Application Under Test) is running
              include_tasks: "/utils/k8s/status_app_pod.yml"
              vars:
                app_ns: "{{namespace}}" 
                app_lkey: "{{ label.split('=')[0] }}"
                app_lvalue: "{{ label.split('=')[1] }}"       
                delay: 5
                retries: 60

            - name: Verify test data
              include: "{{ data_consistency_util_path }}"
              vars:
                status: 'VERIFY'
                ns: "{{ namespace }}"
                pod_name: "{{ app_pod_name.stdout }}"
              when: data_persistence != '' 
          when: stg_prov == 'cstor.csi.openebs.io'
        
        - block:

            - name: Verify that the volume is healthy
              shell: >
                kubectl get jivavolume {{ pv.stdout }} -n {{ operator_ns }} 
                --no-headers -o custom-columns=:.status.phase
              args:
                executable: /bin/bash
              register: Status
              until: "'Ready' in Status.stdout"
              delay: 5
              retries: 60

            - name: Verify that the AUT (Application Under Test) is running
              include_tasks: "/utils/k8s/status_app_pod.yml"
              vars:
                app_ns: "{{namespace}}" 
                app_lkey: "{{ label.split('=')[0] }}"
                app_lvalue: "{{ label.split('=')[1] }}"       
                delay: 5
                retries: 60

            - name: Verify test data
              include: "{{ data_consistency_util_path }}"
              vars:
                status: 'VERIFY'
                ns: "{{ namespace }}"
                pod_name: "{{ app_pod_name.stdout }}"
              when: data_persistence != '' 
          when: stg_prov == 'jiva.csi.openebs.io'

        - set_fact:
            flag: "Pass"

      rescue:
        - set_fact:
            flag: "Fail"

      always:

        ## RECORD END-OF-TEST IN LITMUS RESULT CR

        - include_tasks: /utils/fcm/update_litmus_result_resource.yml
          vars:
            status: 'EOT'
            chaostype: "{{ chaosutil.split('.')[0] }}"
