---
#################### Test involved. ############################
# 1. Check if the application is running already.              #
# 2. Selct any two replicas out of three replicas.             #
# 3. Introduce packet drop in the selected replicas.           #
# 4. Create snapshot.                                          #
# 5. Check if the snapshot is not present in the degraded pod. #
# 6. Bring up the replica.                                     #
# 7. Check if the snapshot is Created succesfully.             #
# 8. Check if the snapshot is rebuilt.                         #
################################################################

- hosts: localhost
  connection: local
  gather_facts: False

  vars_files:
    - test_vars.yml

  tasks:
    - block:

          ## Generating the testname.
        - include_tasks: /utils/fcm/create_testname.yml
  
           ## RECORD START-OF-TEST IN LITMUS RESULT CR
        - include_tasks: "/utils/fcm/update_e2e_result_resource.yml"
          vars:
            status: 'SOT'
       
          ## Obtain cstor replica pods 
        - include_tasks: /funclib/kubectl/k8s_snapshot_clone/snap_rebuild_prerequisites.yml
          vars:
            app_ns: "{{ namespace }}"
            pvc_name: "{{ app_pvc }}" 
            app_label: "{{ application_label }}"
            ns: "{{ operator_ns }}"

        - name: Display details
          debug:
            msg:
              - "cstor_replicas: {{ cstor_replicas_pod }}"

          # #Select any two cstor replica pods to inject packet loss
        - set_fact:
            replica_pod1: "{{ cstor_replicas_pod[0] }}"
            replica_pod2: "{{ cstor_replicas_pod[1] }}"
  
          ## Inducing the packet drop on the above pool pod.
        - include_tasks: "/chaoslib/openebs/inject_packet_loss_tc.yml"
          vars:
            status: "induce"
            target_pod: "{{ item }}"
            operator_namespace: "{{ operator_ns }}"
            containername: "{{ container_name }}"
          with_items:
            - "{{ replica_pod1 }}"
            - "{{ replica_pod2 }}"

          ## If there are no IOs happening on the volume, replica won't get disconnected
        - name: Check if the targeted replica is disconnected.
          shell: >
            kubectl exec -it {{ cstor_target.stdout }} -n {{ operator_ns }} --container  cstor-istgt 
            -- istgtcontrol -q replica |json_pp | jq '.volumeStatus[0].replicaStatus[].Mode' | grep Healthy | wc -l
          register: connected_replica
          until: "(connected_replica.stdout|int) == ((healthy_pods.stdout|int) - 2)"
          delay: 15
          retries: 30

        - name: Creating snapshot.
          include_tasks: /funclib/kubectl/k8s_snapshot_clone/create_snapshot.yml 
          vars:
            app_ns: "{{ namespace }}"
            pvc_name: "{{ app_pvc }}"
  
        - debug:
            msg: "snapshotname: {{ snapshot_name }}"
              
          ## snapsot is not created successfully when the  connected replica is not met with quorum
        - name: Checking the status of snapshot
          include_tasks: /funclib/kubectl/k8s_snapshot_clone/snapshot_status.yml 
          vars:
            app_ns: "{{ namespace }}"
            pvc_name: "{{ app_pvc }}"
            snap_name: "{{ snapshot_name }}"
            action: 'Failure'

        - name: Checking if the snapshot is not created in the degraded replica.
          shell: kubectl exec -ti {{ item }} -n {{ operator_ns }} --container cstor-pool -- zfs list -t snapshot
          args:
            executable: /bin/bash
          register: list_snap
          failed_when: "snapshot_name in list_snap.stdout"
          with_items:
            - "{{ replica_pod1 }}"
            - "{{ replica_pod2 }}"

        ## Removing the packet drop rule by including the relevant util..
        - include_tasks: "/chaoslib/openebs/inject_packet_loss_tc.yml"
          vars:
            status: "remove"
            target_pod: "{{ item }}"
            operator_namespace: "{{ operator_ns }}"
            containername: "{{ container_name }}"
          with_items:
            - "{{ replica_pod1 }}"
            - "{{ replica_pod2 }}"

        - name: Checking the snapshot status after removing packet loss
          include_tasks: /funclib/kubectl/k8s_snapshot_clone/snapshot_status.yml
          vars:
            app_ns: "{{ namespace }}"
            snap_name: "{{ snapshot_name }}"
            pvc_name: "{{ app_pvc }}"
            action: 'Success'

        - debug:
            msg: "snapid_name: {{ snapid_name }}"

        - name: Check if the snapshot is rebuilt.
          shell: kubectl exec -it {{ item }} -n {{ operator_ns }} --container cstor-pool -- zfs list -t snapshot
          args:
            executable: /bin/bash
          register: list_snap
          until: "snapid_name in list_snap.stdout"
          delay: 30
          retries: 20
          with_items:
            - "{{ replica_pod1 }}"
            - "{{ replica_pod2 }}"

        - name: Check if the snapshot rebuild is successful.
          shell: >
             kubectl exec -ti {{ item }} -n {{ operator_ns }} -c cstor-pool 
             -- zfs stats |json_pp| grep "rebuildStatus" |awk -F ':' '{print $2}' | tr -d '"' | tr -d ','
          args:
            executable: /bin/bash
          register: snap_result
          until: "'DONE' in snap_result.stdout"
          delay: 20
          retries: 100
          with_items:
            - "{{ replica_pod1 }}"
            - "{{ replica_pod2 }}"          

        - name: Delete the snapshot that created
          shell: >
            kubectl delete volumesnapshots {{ snapshot_name }} -n {{ namespace }}
          args:
            executable: /bin/bash

        - name: Check if the snapshot has been deleted
          shell: >
             kubectl get volumesnapshot -n {{ namespace }}
          args:
            executable: /bin/bash
          register: snap_list
          until: "snapshot_name not in snap_list.stdout"
          delay: 10
          retries: 15

        - set_fact:
            flag: "Pass"

      rescue:
        - set_fact:
            flag: "Fail"

      always:
            ## RECORD END-OF-TEST IN LITMUS RESULT CR
        - include_tasks: /utils/fcm/update_e2e_result_resource.yml
          vars:
            status: 'EOT'              
