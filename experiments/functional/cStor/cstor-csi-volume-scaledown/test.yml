- hosts: localhost
  connection: local

  vars_files:
    - test_vars.yml
    - /mnt/parameters.yml
    
  tasks:

    - block:

        ## Generating the testname for deployment
        - include_tasks: /utils/fcm/create_testname.yml

        ## RECORD START-OF-TEST IN LITMUS RESULT CR
        - include_tasks: "/utils/fcm/update_litmus_result_resource.yml"
          vars:
            status: 'SOT'

        - name: Identify the data consistency util to be invoked
          template:
            src: data_persistence.j2
            dest: data_persistence.yml

        - include_vars:
            file: data_persistence.yml

        - name: Record the data consistency util path
          set_fact:
            data_consistency_util_path: "{{ consistencyutil }}"
          when: data_persistence != ''
        
        - name: Get the application pod name
          shell: >
            kubectl get pod -n {{ namespace }} --no-headers -o custom-columns=:.metadata.name
          args:
            executable: /bin/bash
          register: app_pod_name

        - name: Check if the application pod is in running state
          shell: >
            kubectl get pods -n {{ namespace }} --no-headers -o custom-columns=:.status.phase
          args:
            executable: /bin/bash
          register: pod_status
          failed_when: "'Running' not in pod_status.stdout"

        - name: Generate data on the specified application.
          include: "{{ data_consistency_util_path }}"
          vars:
            status: 'LOAD'
            ns: "{{ namespace }}"
            pod_name: "{{ app_pod_name.stdout }}"
          when: data_persistence != ''

        - name: Derive PV name from PVC.
          shell: >
            kubectl get pvc "{{ pvc_name }}" -n "{{ namespace }}"
            --no-headers -o custom-columns=:spec.volumeName
          args:
            executable: /bin/bash
          register: pv

        - name: Get CVC name, note that CVC name is same as PV.
          set_fact:
            cvc_name: "{{ pv.stdout }}"

        - name: Obtain the CVC manifest into two files
          shell: >
            kubectl get cvc "{{ cvc_name }}" -n "{{ openebs_ns }}" -o yaml | tee cvc1.yml cvc2.yml
          args:
            executable: /bin/bash
          register: result
          failed_when: "result.rc != 0"

        - name: Get the current replica count
          shell: >
            kubectl get cstorvolume "{{ cvc_name }}" -n "{{ openebs_ns }}" --no-headers 
            -o custom-columns=:spec.replicationFactor
          args:
            executable: /bin/bash
          register: current_replica

        - name: Remove couple of pool instances from cvc manifest
          shell: sed -i '/replicaPoolInfo\:/{n;N;N;d}' cvc2.yml
          args:
            executable: /bin/bash

        ## openebs admission server should block the scale down process because only one replica should be aloowed to delete.
      
        - name: Try to scale down couple of replicas by applying above manifest
          shell: kubectl apply -f cvc2.yml
          args:
            executable: /bin/bash
          register: state
          failed_when: state.rc == 0

        - name: Remove one pool instance info from cvc manifest
          shell: sed -i '/replicaPoolInfo:/{N;s/\n.*//;}' cvc1.yml
          args:
            executable: /bin/bash
          
        - name: Scale down the replicas by configuring cvc
          shell: kubectl apply -f cvc1.yml
          args:
            executable: /bin/bash
          register: result
          failed_when: result.rc != 0

        - name: Check the replica count
          shell: >
            kubectl get cstorvolume "{{ cvc_name }}" -n "{{ openebs_ns }}" --no-headers
            -o custom-columns=:spec.replicationFactor
          args:
            executable: /bin/bash
          register: updated_replica
          until: "current_replica.stdout > updated_replica.stdout"
          delay: 10
          retries: 20

        - name: Delete the application pod
          shell: kubectl delete pod "{{ app_pod_name.stdout }}" -n "{{ namespace }}"
          args:
            executable: /bin/bash
          register: result
          failed_when: "result.rc != 0"

        - name: Get the application pod name
          shell: >
            kubectl get pod -n {{ namespace }} --no-headers -o custom-columns=:.metadata.name
          args:
            executable: /bin/bash
          register: rescheduled_pod_name

        - name: Check if the application pod is in running state
          shell: >
            kubectl get pods "{{ rescheduled_pod_name.stdout }}" -n "{{ namespace }}" 
            --no-headers -o custom-columns=:.status.phase
          args:
            executable: /bin/bash
          register: pod_status
          until: "'Running' in pod_status.stdout"
          delay: 10
          retries: 30

        - name: Verify application data persistence
          include: "{{ data_consistency_util_path }}"
          vars:
            status: 'VERIFY'
            ns: "{{ namespace }}"
            pod_name: "{{ rescheduled_pod_name.stdout }}"
          when: data_persistence != ''

        - set_fact:
            flag: "Pass"

      rescue:
        - set_fact:
            flag: "Fail"

      always:
            ## RECORD END-OF-TEST IN LITMUS RESULT CR
        - include_tasks: /utils/fcm/update_litmus_result_resource.yml
          vars:
            status: 'EOT'
    
