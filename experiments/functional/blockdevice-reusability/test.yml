---
- hosts: localhost
  connection: local
  gather_facts: False

  vars_files:
    - test_vars.yml

  tasks:
    - block:

         ## Generating the testname for deployment
        - include_tasks: /utils/fcm/create_testname.yml

         ## RECORD START-OF-TEST IN LITMUS RESULT CR
        - include_tasks: "/utils/fcm/update_litmus_result_resource.yml"
          vars:
            status: 'SOT'

        - name: Obtain the node name from the BDC of SPC pool
          shell: >
            kubectl get bdc -n {{ operator_ns }} -l openebs.io/storage-pool-claim={{ spc_pool_name }} 
            -o custom-columns=:.spec.blockDeviceNodeAttributes.hostName --no-headers
          args:
            executable: /bin/bash
          register: nodes

        # Creating the blockdevice template from blockdevice.j2 jinja template for each node
        - name: Add node labels for each nodes and create blockdevice template
          template:
            src: ./blockdevice.j2
            dest: ./blockdevice-{{ item[0] }}.yml
          with_together:
            - "{{ nodes.stdout_lines }}"

        - name: Add the block devices for each node's block device template
          include_tasks: add_blockdevice.yml
          with_items: "{{ nodes.stdout_lines }}"
          loop_control:
            loop_var: outer_item

        # Insert the blockdevice template created for each nodes into cspc spec
        # blockinfile module will insert the external_files/block.
        # marker line template will be replaced with the values in marker_begin (default="BEGIN") and marker_end (default="END").
        - name: Include the blockdevice template in the CSPC spec
          blockinfile:
            dest: ./cspc.yml
            marker_end: "## {{ item }} Ansible Config ##"
            insertafter: pools
            state: present
            block: |
              {{ lookup('file', './blockdevice-{{ item }}.yml') }}
          with_items:
            - "{{ nodes.stdout_lines }}"

        - name: Replacing the pool name in CSPC spec
          replace:
            path: ./cspc.yml
            regexp: "cspc-pool-name"
            replace: "{{ cspc_pool_name }}"

        - name: Replacing the namespace in CSPC spec
          replace:
            path: ./cspc.yml
            regexp: "operator_ns"
            replace: "{{ operator_ns }}"

        - name: Replacing the pool type in CSPC spec
          replace:
            path: ./cspc.yml
            regexp: "pool-type"
            replace: "{{ pool_type }}"

        - name: Display cspc.yml for verification
          debug: var=item
          with_file:
          - "cspc.yml"

        - name: Obtain the claimed blockDevice list from bdc
          shell: >
            kubectl get bdc -n {{ operator_ns }} -l openebs.io/storage-pool-claim={{ spc_pool_name }}
            -o custom-columns=:.spec.blockDeviceName --no-headers
          args:
            executable: /bin/bash
          register: blockdevice_name

        - name: Remove the SPC
          shell: kubectl delete spc {{ spc_pool_name }}
          args:
            executable: /bin/bash
          
        - name: Verify if the SPC is deleted
          shell: kubectl get spc
          args:
            executable: /bin/bash
          register: spc_status
          until: '"{{ spc_pool_name }}" not in spc_status.stdout'
          retries: 30
          delay: 10

        - name: Verify if the CSP is getting removed
          shell: kubectl get csp -n {{ operator_ns }} -l openebs.io/cstor-pool-cluster={{ spc_pool_name }}
          args:
            executable: /bin/bash
          register: csp_status
          until: "'No resources found.' in csp_status.stderr"
          retries: 30
          delay: 10

        - name: Verify if the cStor pool pods are deleted
          shell: kubectl get pods -n {{ operator_ns }} -l openebs.io/storage-pool-claim={{ spc_pool_name }}
          args:
            executable: /bin/bash
          register: pool_status
          until: "'No resources found.' in pool_status.stderr"
          retries: 30
          delay: 10

        - name: Verify if the BDC are deleted
          shell: kubectl get bdc -n {{ operator_ns }} -l openebs.io/storage-pool-claim={{ spc_pool_name }}
          args:
            executable: /bin/bash
          register: bdc_status
          until: "'No resources found.' in bdc_status.stderr"
          retries: 30
          delay: 10

        - name: Verify if the blockDevices are unclaimed
          shell: >
            kubectl get blockdevices -n {{ operator_ns }} 
            -o jsonpath='{.items[?(@.metadata.name=="{{ item }}")].status.claimState}'
          args:
            executable: /bin/bash
          register: bd_status
          with_items: "{{ blockdevice_name.stdout_lines }}"
          until: "'Unclaimed' in bd_status.stdout"
          retries: 30
          delay: 10

        - name: Create cstor cspc disk pool
          shell: kubectl apply -f cspc.yml
          args:
            executable: /bin/bash

        - name: Check whether cspc is created
          shell: >
            kubectl get cspc -n {{ operator_ns }} -o custom-columns=:.metadata.name --no-headers
          args:
            executable: /bin/bash
          register: cspc_name
          until: "pool_name in cspc_name.stdout"
          delay: 30
          retries: 10

        - name: Verify the status of CSPI
          shell: >
            kubectl get cspi -n {{ operator_ns}} -l openebs.io/cstor-pool-cluster={{ cspc_pool_name }}
            -o custom-columns=:.status.phase --no-headers
          args:
            executable: /bin/bash
          register: cspi_status
          until: "((cspi_status.stdout_lines|unique)|length) == 1 and 'ONLINE' in cspi_status.stdout"
          delay: 5
          retries: 60

        - name: Obtain the CSPI name to verify the status
          shell: >
            kubectl get cspi -n {{ operator_ns}} -l openebs.io/cstor-pool-cluster={{ cspc_pool_name }}
            -o custom-columns=:.metadata.name --no-headers
          args:
            executable: /bin/bash
          register: cspi_name

        - name: Verify if the cStor Pool pods are Running
          shell: >
            kubectl get pods -n {{ operator_ns }} -l openebs.io/cstor-pool-instance={{ item }}
            --no-headers -o custom-columns=:status.phase
          args:
            executable: /bin/bash
          register: poolcount
          with_items: "{{ cspi_name.stdout_lines }}"
          until: "((poolcount.stdout_lines|unique)|length) == 1 and 'Running' in poolcount.stdout"
          retries: 30
          delay: 10

        - name: Get the runningStatus of pool pod
          shell: >
            kubectl get pod {{ item }} -n {{ operator_ns }}
            -o=jsonpath='{range .status.containerStatuses[*]}{.state}{"\n"}{end}' |
            grep -w running | wc -l
          args:
            executable: /bin/bash
          register: runningStatusCount
          with_items: "{{ cstor_pool_pod.stdout_lines }}"
          until: "runningStatusCount.stdout == 3"
          delay: 30
          retries: 10
            
        - name: Remove the cStor Pool Cluster
          shell: >
            kubectl delete cspc {{ cspc_pool_name }} -n {{ operator_ns }}
          args:
            executable: /bin/bash

        - name: Verify if the cStor Pool Cluster is deleted
          shell: >
            kubectl get cspc -n {{ operator_ns }}
          register: cspc_status
          until: '"{{ cspc_pool_name }}" not in cspc_status.stdout'
          retries: 30
          delay: 10

        - name: Verify if the CSPI is getting removed
          shell: >
            kubectl get cspi -n {{ operator_ns }} -l openebs.io/cstor-pool-cluster={{ cspc_pool_name }}
          args:
            executable: /bin/bash
          register: cspi_status
          until: "'No resources found.' in cspi_status.stderr"
          retries: 30
          delay: 10

        - name: Verify if the cStor pool pods are deleted
          shell:
            kubectl get pods -n {{ operator_ns }} -l openebs.io/cstor-pool-cluster={{ cspc_pool_name }}
          args:
            executable: /bin/bash
          register: pool_status
          until: "'No resources found.' in pool_status.stderr"
          retries: 30
          delay: 10

        - set_fact:
            flag: "Pass"

      rescue:
          - set_fact:
              flag: "Fail"

      always:
            ## RECORD END-OF-TEST IN LITMUS RESULT CR
          - include_tasks: /utils/fcm/update_litmus_result_resource.yml
            vars:
              status: 'EOT'