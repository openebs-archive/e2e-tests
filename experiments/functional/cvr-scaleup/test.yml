---
- hosts: localhost
  connection: local

  vars_files:
    - test_vars.yml
    - /mnt/parameters.yml
  
  tasks:
    - block:

          ## Generating the testname for deployment
        - include_tasks: /utils/fcm/create_testname.yml

          ## RECORD START-OF-TEST IN LITMUS RESULT CR
        - include_tasks: /utils/fcm/update_litmus_result_resource.yml
          vars:
            status: 'SOT'
        
        - name: Identify the data consistency util to be invoked
          template:
            src: data_persistence.j2
            dest: data_persistence.yml

        - include_vars:
            file: data_persistence.yml
        
        - name: Record the data consistency util path
          set_fact:
            data_consistency_util_path: "{{ consistencyutil }}"
          when: data_persistence != '' 
      
        - name: Checking the status of application pod
          shell: kubectl get pod -n {{ app_ns }} -l {{ label }} -o jsonpath='{.items[0].status.phase}'
          register: app_pod_status
          until: "'Running' in app_pod_status.stdout"
          delay: 5
          retries: 30

        - name: Getting the pod name of Application
          shell: kubectl get pod -n {{ app_ns }} -l {{ label }} -o jsonpath='{.items[0].metadata.name}'
          register: app_pod_name

        - block:

            - name: Getting the application mount point
              shell: kubectl get pod {{ app_pod_name.stdout }} -n {{ app_ns }} -o jsonpath='{.spec.containers[0].volumeMounts[0].mountPath}'
              register: app_mount_path
              
              #Writing approx 1.6G of data at mount point
            - name: Writing data on application mount point
              shell: kubectl exec -it {{ app_pod_name.stdout }} -n {{ app_ns }} -- sh -c "cd {{ app_mount_path.stdout }} && dd if=/dev/urandom of=test.txt bs=4k count=400024"

          when: lookup('env','INDUCE_TARGET_FAILURE') == 'enable'
         

        - name: Geting the PV name
          shell: kubectl get pvc {{ app_pvc }} -n {{ app_ns }} -o jsonpath='{.spec.volumeName}'
          register: pv
          until: "pv is defined"
          delay: 5
          retries: 10

        - name: Checking the cstorvolume status
          shell: kubectl get cstorvolume -n {{ operator_ns }} {{ pv.stdout }} -o jsonpath='{.status.phase}'
          register: cstor_volume_status
          until: "'Healthy' in cstor_volume_status.stdout"
          delay: 5
          retries: 30

        - name: Verify cvr health status
          shell: kubectl get cvr -n {{ operator_ns }} -l openebs.io/persistent-volume={{ pv.stdout }} -o jsonpath='{.items[*].status.phase}'
          register: initial_status
          until: "((initial_status.stdout.split()|unique)|length) == 1 and 'Healthy' in initial_status.stdout"
          delay: 3
          retries: 30
        
        - name: Create some test data
          include: "{{ data_consistency_util_path }}"
          vars:
            status: 'LOAD'
            ns: "{{ app_ns }}"
            pod_name: "{{ app_pod_name.stdout }}"  
          when: data_persistence != ''
        
        - name: Get the storage class name
          shell: >
            kubectl get pvc {{ app_pvc }} -n {{ app_ns }} --no-headers -o custom-columns=:.spec.storageClassName
          register: sc_name

        - name: Get the storage pool claim (spc) name
          shell: >
            kubectl get sc {{ sc_name.stdout }} -o jsonpath="{.metadata.annotations.cas\.openebs\.io\/config}" | awk '/StoragePoolClaim/{getline; print}' | cut -d ':' -f2 | sed 's/"//g' | sed 's/ //g'
          register: spc_name

        - name: Getting the list of csp name correspond to provided spc
          shell: kubectl get csp -l openebs.io/storage-pool-claim={{ spc_name.stdout }} -o jsonpath='{.items[*].metadata.name}'
          register: csp_all

        - name: Create a file to store CSP name
          lineinfile:
            create: yes
            state: present
            path: "./csp_all.tmp"
            line: "{{ item.stdout }}"
            mode: 0755
          with_items:
            - "{{ csp_all }}"

        - name: Getting the list of csp on which cvr is created
          shell:  kubectl get cvr -n {{ operator_ns }} -l cstorvolume.openebs.io/name={{ pv.stdout }} -o jsonpath='{.items[*].metadata.labels.cstorpool\.openebs\.io/name}'
          register: csp_cvr

        - name: Create a file to store CSP name
          lineinfile:
            create: yes
            state: present
            path: "./csp_cvr.tmp"
            line: "{{ item.stdout }}"
            mode: 0755
          with_items:
            - "{{ csp_cvr }}"

        - name: Getting the csp where no cvr is created
          shell: |
            cat csp_all.tmp | tr " " "\n" > csp_all
            cat csp_cvr.tmp | tr " " "\n" > csp_cvr
            comm -3 <(sort csp_all) <(sort csp_cvr)
          args: 
            executable: /bin/bash
          register: csp_name

        - name: Select random csp from the list of csp as target csp
          set_fact:
            target_csp: "{{ item }}"
          with_random_choice: "{{ csp_name.stdout_lines }}"

        - name: Getting the target IP from cvr
          shell: kubectl get cvr -n {{ operator_ns }} -l cstorvolume.openebs.io/name={{ pv.stdout }} -o jsonpath='{.items[0].spec.targetIP}'
          register: target_ip

        - name: Getting the Node name from CSP where CVR need to create
          shell: kubectl get csp {{ csp_name.stdout_lines[0] }} -o jsonpath='{.metadata.labels.kubernetes\.io/hostname}'
          register: node_name

        - name: Getting csp uid 
          shell: kubectl get csp {{ csp_name.stdout_lines[0] }} -o jsonpath='{.metadata.uid}'
          register: uid

        - name: Replacing the storage class in cvr yml
          replace:
            path: "./cvr.yml"
            regexp: "<storage-class>"
            replace: "openebs-cstor-cvr-scale"
        
        - name: Replacing the Node name in cvr yml
          replace:
            path: "./cvr.yml"
            regexp: "<node_name>"
            replace: "{{ node_name.stdout }}"

        - name: Replacing the CSP name in cvr yml
          replace:
            path: "./cvr.yml"
            regexp: "<csp-name>"
            replace: "{{ csp_name.stdout_lines[0] }}"

        - name: Replacing the uid in cvr yml
          replace:
            path: "./cvr.yml"
            regexp: "<csp-uid>"
            replace: "{{ uid.stdout }}"
          
        - name: Replacing the volume name in cvr yml
          replace:
            path: "./cvr.yml"
            regexp: "<cstor-volume-name>"
            replace: "{{ pv.stdout }}"

        - name: Replacing the volume capacity in cvr yml
          replace:
            path: "./cvr.yml"
            regexp: "<initial_capacity>"
            replace: "{{ lookup('env','PV_CAPACITY') }}"

        - name: Replacing OpenEBS version in cvr yml
          replace:
            path: "./cvr.yml"
            regexp: "<openebs-version>"
            replace: "{{ lookup('env','OPENEBS_VERSION') }}"

        - name: Replacing the target IP in cvr yml
          replace:
            path: "./cvr.yml"
            regexp: "<target-ip>"
            replace: "{{ target_ip.stdout }}"

        - name: Replacing the DRF in patch_cstorvolume yml
          replace:
            path: "./patch_cstorvolume.yml"
            regexp: "<desired-replication-factor>"
            replace: "3"

        - name: Patching the cstorvolume's desired rep count
          shell: kubectl patch cstorvolume {{ pv.stdout }} -n {{ operator_ns }} --type merge --patch "$(cat patch_cstorvolume.yml)"
          register: patch_cstorvolume_result
          failed_when: "'patched' not in patch_cstorvolume_result.stdout"
        
        - name: Creating new cvr
          shell: kubectl apply -f cvr.yml

        - name: Getting the uid of newly created cvr
          shell: kubectl get cvr -n {{ operator_ns }} -l cstorpool.openebs.io/name={{ target_csp }},cstorvolume.openebs.io/name={{ pv.stdout }} -o jsonpath='{.items[0].metadata.uid}'
          register: new_uid

        - name: Creating MD5SUM of new-uid
          shell: echo {{ new_uid.stdout }} | md5sum | awk '{print toupper($1)}' 
          register: md5sum

        - name: Replacing the md5sum value in patch yml
          replace:
            path: "./patch_cvr.yml"
            regexp: "<md5-sum-uid>"
            replace: "{{ md5sum.stdout }}"

        - name: Patching the cvr's replica id with md5sum
          shell: kubectl patch cvr {{ pv.stdout }}-{{ target_csp }} -n {{ operator_ns }} --type merge --patch "$(cat patch_cvr.yml)"
          register: patch_cvr_result
          failed_when: "'patched' not in patch_cvr_result.stdout"
    
        - name: Getting the cstor pool pod name
          shell: kubectl get pod -n {{ operator_ns }} -l app=cstor-pool,openebs.io/cstor-pool={{ target_csp }} -o jsonpath='{.items[0].metadata.name}'
          register: pool_pod_name

        - name: Restarting the cstor-pool-mgmt container
          shell: kubectl exec -it {{ pool_pod_name.stdout }} -n {{ operator_ns }} -c cstor-pool-mgmt -- pkill -f /usr/local/bin/
          ignore_errors: true

        - name: Checking for the pool pod to be in running state
          shell: kubectl get pod -n {{ operator_ns }} -l app=cstor-pool,openebs.io/cstor-pool={{ target_csp }} -o jsonpath='{.items[0].status.phase}'
          register: pod_status
          until: "((pod_status.stdout_lines|unique)|length) == 1 and 'Running' in pod_status.stdout"
          delay: 10
          retries: 50

        - block: 

            - name: Getting the target name 
              shell:  kubectl get pod -n {{ operator_ns }} -l openebs.io/persistent-volume={{ pv.stdout }} -o jsonpath='{.items[0].metadata.name}' 
              register: target_name

            - name: Checking for reconstructing state of newly created cvr 
              shell: kubectl get cvr {{ pv.stdout }}-{{ csp_name.stdout_lines[0] }} -n {{ operator_ns }} -o jsonpath='{.status.phase}'
              register: new_cvr_state
              until: "'ReconstructingNewReplica' in new_cvr_state.stdout"
              delay: 10
              retries: 100

            - name: Restarting target
              shell: kubectl delete pod {{ target_name.stdout }} -n {{ operator_ns }}
              register: target_delete_status
              failed_when: "'deleted' not in target_delete_status.stdout"

            - name: Verifying successful deletion of target
              shell: kubectl get pod -n {{ operator_ns }} -l openebs.io/persistent-volume={{ pv.stdout }} -o jsonpath='{.items[0].metadata.name}'
              register: target_del
              until: "'{{ target_name.stdout }}' not in target_del.stdout"
              delay: 10
              retries: 100

            - name: Waiting for new target to be in running state
              shell: kubectl get pod -n {{ operator_ns }} -l openebs.io/persistent-volume={{ pv.stdout }} -o jsonpath='{.items[0].status.phase}'
              register: new_target_state
              until: "'Running' in new_target_state.stdout"
              delay: 10
              retries: 100
               
          when: lookup('env','INDUCE_TARGET_FAILURE') == 'enable'

        - name: Verify newly Created replicaID exists in cstorVolume or not
          shell: kubectl get cstorvolume {{ pv.stdout }} -n {{ operator_ns }} -o jsonpath='{.status.replicaDetails.knownReplicas.{{ md5sum.stdout }}}'
          register: replica_status
          until: "replica_status.stdout != ''"
          delay: 10
          retries: 100

        - name: Checking for the desired replication factor in cstorvolume
          shell: kubectl get cstorvolume {{ pv.stdout }} -n {{ operator_ns }} -o jsonpath='{.spec.replicationFactor}'
          register: replication_factor
          failed_when: "'3' not in replication_factor.stdout"

        - name: Checking for the consistancy factor in cstorvolume
          shell: kubectl get cstorvolume {{ pv.stdout }} -n {{ operator_ns }} -o jsonpath='{.spec.consistencyFactor}'
          register: consistancy_factor
          failed_when: "'2' not in consistancy_factor.stdout"

        - name: Verify cvr health status
          shell: kubectl get cvr -n openebs -l openebs.io/persistent-volume={{ pv.stdout }} -o jsonpath='{.items[*].status.phase}'
          register: final_status
          until: "((final_status.stdout.split()|unique)|length) == 1 and 'Healthy' in final_status.stdout"
          delay: 10
          retries: 100

        - name: Get istgt target pod details
          shell: >
            kubectl get pods  -l openebs.io/persistent-volume={{ pv.stdout }} 
            -n {{ operator_ns }} -o custom-columns=:metadata.name --no-headers
          register: istgt_replica
          failed_when: 'istgt_replica.stdout == ""'

        - name: Create an empty list of replica pods.
          set_fact:
            cstor_replicas_pod : []         

        - name: Obtaining the pool deployments from cvr
          shell: >
              kubectl get cvr -n {{ operator_ns }}
              -l openebs.io/persistent-volume={{ pv.stdout }} --no-headers
              -o=jsonpath='{range .items[*]}{.metadata.labels.cstorpool\.openebs\.io\/name}{"\n"}{end}'
          args:
              executable: /bin/bash
          register: pool_deployment
          failed_when: 'pool_deployment.stdout == ""'
  
        - name: Obtaining the pool pods
          shell: >
            kubectl get pod -l app=cstor-pool,openebs.io/cstor-pool={{ item }} 
            -n {{ operator_ns }} --no-headers -o custom-columns=:.metadata.name
          register: pool_pods
          failed_when: 'pool_pods.stdout == ""'
          with_items:
            - "{{ pool_deployment.stdout_lines }}"
  
        - name: Build a list of replica pods
          set_fact:
            cstor_replicas_pod : "{{ cstor_replicas_pod }} + [ '{{ item.stdout }}' ]"
          with_items: "{{ pool_pods.results }}"
  
        - name: Generate snapshot name
          shell: cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1
          register: snapname
  
        - set_fact:
            snap_name: "{{ snapname.stdout }}"
  
        - name: Take snapshot for data verification
          shell: >
              kubectl exec -it {{ istgt_replica.stdout }} -n {{ operator_ns }} --container cstor-istgt -- istgtcontrol snapcreate {{ pv.stdout }} {{ snap_name }} 30 30
          register: snap_status
          until: "'DONE SNAPCREATE command' in snap_status.stdout"
          delay: 20
          retries: 10
  
        - name: Install netcat on current host
          shell: apt-get update && apt-get -y install netcat iproute2
          register: dataset
  
        - include: /chaoslib/openebs/fetch_data_from_replica.yml
          loop: "{{ cstor_replicas_pod }}"
          loop_control:
            loop_var: rpod       
  
        - set_fact:
              base_replica: "{{ cstor_replicas_pod | list | first }}"
  
        - name: compare data object
          shell: diff {{ rpod }}.1.dump {{ base_replica }}.1.dump
          loop: "{{ cstor_replicas_pod }}"
          loop_control:
            loop_var: rpod
  
        - name: compare meta data object
          shell: diff {{ rpod }}.3.dump {{ base_replica }}.3.dump
          loop: "{{ cstor_replicas_pod }}"
          loop_control:
              loop_var: rpod
  
        - name: Destroy snapshot created for data verification
          shell: >
            kubectl exec -it {{ istgt_replica.stdout }} -n {{ operator_ns }} --container  cstor-istgt -- istgtcontrol snapdestroy {{ pv.stdout }} {{ snap_name }} 30 30
          register: snap_status                            

        - name: Verify application data persistence  
          include: "{{ data_consistency_util_path }}"
          vars:
            status: 'VERIFY'
            ns: "{{ app_ns }}"
            pod_name: "{{ app_pod_name.stdout }}"
          when: data_persistence != ''

        - set_fact:
            flag: "Pass"

      rescue:
        - name: Setting fail flag
          set_fact:
            flag: "Fail"  

      always:
    ## RECORD END-OF-TEST IN LITMUS RESULT CR
        - include_tasks: /utils/fcm/update_litmus_result_resource.yml
          vars:
            status: 'EOT'
